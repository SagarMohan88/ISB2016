library(e1071)
data()
data(iris)
print(head(iris))
View(iris)
tail(iris)
res = naiveBayes(iris[,1:4],iris[,5])
res
predict(res,iris[99,1:4],type="raw")
predict(res,iris[,1:4],type="raw")
out = table(predict(res,iris[,1:4]),iris[,5])
out
library(e1071)
model = svm(iris[,1:4],iris[,5])
model
out = predict(model,iris[,1:4])
out
print(length(out))
table(matrix(out),iris[,5])
library(rvest)
url = "http://srdas.github.io/bio-candid.html"
doc.html = read_html(url)
text = doc.html %>% html_nodes("p") %>% html_text()
text = gsub("[\t\n]"," ",text)
text = gsub('"'," ",text)   #removes single backslash
text = paste(text, collapse=" ")
print(text)
library(koRpus)
write(text,file="textvec.txt")
text_tokens = tokenize("textvec.txt",lang="en")
print(text_tokens)
print(c("Number of sentences: ",text_tokens@desc$sentences))
print(c("Number of words: ",text_tokens@desc$words))
print(c("Number of words per sentence: ",text_tokens@desc$avg.sentc.length))
print(c("Average length of words: ",text_tokens@desc$avg.word.length))
print(readability(text_tokens))
library(tm)
library(stringr)
#READ IN TEXT FOR ANALYSIS, PUT IT IN A CORPUS, OR ARRAY, OR FLAT STRING
#cstem=1, if stemming needed
#cstop=1, if stopwords to be removed
#ccase=1 for lower case, ccase=2 for upper case
#cpunc=1, if punctuation to be removed
#cflat=1 for flat text wanted, cflat=2 if text array, else returns corpus
read_web_page = function(url,cstem=0,cstop=0,ccase=0,cpunc=0,cflat=0) {
text = readLines(url)
text = text[setdiff(seq(1,length(text)),grep("<",text))]
text = text[setdiff(seq(1,length(text)),grep(">",text))]
text = text[setdiff(seq(1,length(text)),grep("]",text))]
text = text[setdiff(seq(1,length(text)),grep("}",text))]
text = text[setdiff(seq(1,length(text)),grep("_",text))]
text = text[setdiff(seq(1,length(text)),grep("\\/",text))]
ctext = Corpus(VectorSource(text))
if (cstem==1) { ctext = tm_map(ctext, stemDocument) }
if (cstop==1) { ctext = tm_map(ctext, removeWords, stopwords("english"))}
if (cpunc==1) { ctext = tm_map(ctext, removePunctuation) }
if (ccase==1) { ctext = tm_map(ctext, tolower) }
if (ccase==2) { ctext = tm_map(ctext, toupper) }
text = ctext
#CONVERT FROM CORPUS IF NEEDED
if (cflat>0) {
text = NULL
for (j in 1:length(ctext)) {
temp = ctext[[j]]$content
if (temp!="") { text = c(text,temp) }
}
text = as.array(text)
}
if (cflat==1) {
text = paste(text,collapse="\n")
text = str_replace_all(text, "[\r\n]" , " ")
}
result = text
}
# FUNCTION TO RETURN n SENTENCE SUMMARY
# Input: array of sentences (text)
# Output: n most common intersecting sentences
text_summary = function(text, n) {
m = length(text)  # No of sentences in input
jaccard = matrix(0,m,m)  #Store match index
for (i in 1:m) {
for (j in i:m) {
a = text[i]; aa = unlist(strsplit(a," "))
b = text[j]; bb = unlist(strsplit(b," "))
jaccard[i,j] = length(intersect(aa,bb))/
length(union(aa,bb))
jaccard[j,i] = jaccard[i,j]
}
}
similarity_score = rowSums(jaccard)
res = sort(similarity_score, index.return=TRUE,
decreasing=TRUE)
idx = res$ix[1:n]
summary = text[idx]
}
url = "data_files/dstext_sample.txt"   #You can put any text file or URL here
text = read_web_page(url,cstem=0,cstop=0,ccase=0,cpunc=0,cflat=1)
print(length(text[[1]]))
print("ORIGINAL TEXT")
print(text)
text2 = strsplit(text,". ",fixed=TRUE)  #Special handling of the period.
text2 = text2[[1]]
print("SENTENCES")
print(text2)
res = text_summary(text2,5)
print(res)
system("mkdir D")
write( c("blue", "red", "green"), file=paste("D", "D1.txt", sep="/"))
write( c("black", "blue", "red"), file=paste("D", "D2.txt", sep="/"))
write( c("yellow", "black", "green"), file=paste("D", "D3.txt", sep="/"))
write( c("yellow", "red", "black"), file=paste("D", "D4.txt", sep="/"))
library(lsa)
tdm = textmatrix("D",minWordLength=1)
print(tdm)
system("rm -rf D")
et = eigen(tdm %*% t(tdm))$vectors
print(et)
ed = eigen(t(tdm) %*% tdm)$vectors
print(ed)
res = lsa(tdm,dims=dimcalc_share())
print(res)
res2 = svd(tdm)
print(res2)
tdm_lsa = res$tk %*% diag(res$sk) %*% t(res$dk)
print(tdm_lsa)
library(Matrix)
print(rankMatrix(tdm))
print(rankMatrix(tdm_lsa))
library(topicmodels)
data(AssociatedPress)
print(dim(AssociatedPress))
dtm = AssociatedPress[1:100,]
dim(dtm)
#Set parameters for Gibbs sampling
burnin = 4000
iter = 2000
thin = 500
seed = list(2003,5,63,100001,765)
nstart = 5
best = TRUE
#Number of topics
k = 5
#Set parameters for Gibbs sampling
burnin = 4000
iter = 2000
thin = 500
seed = list(2003,5,63,100001,765)
nstart = 5
best = TRUE
#Number of topics
k = 5
res <-LDA(dtm, k, method="Gibbs", control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin))
res.topics = as.matrix(topics(res))
print(res.topics)
res.terms = as.matrix(terms(res,10))
print(res.terms)
library(text2vec)
library(data.table)
data("movie_review")
setDT(movie_review)
setkey(movie_review, id)
set.seed(2016L)
all_ids = movie_review$id
train_ids = sample(all_ids, 4000)
test_ids = setdiff(all_ids, train_ids)
train = movie_review[J(train_ids)]
test = movie_review[J(test_ids)]
print(head(train))
View(train)
prep_fun = tolower
tok_fun = word_tokenizer
#Create an iterator to pass to the create_vocabulary function
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$id,
progressbar = FALSE)
#Now create a vocabulary
vocab = create_vocabulary(it_train)
print(vocab)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
print(dim(as.matrix(dtm_train)))
res = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
alpha = 1,
type.measure = "auc",
nfolds = NFOLDS,
thresh = 1e-3,
maxit = 1e3)
library(glmnet)
NFOLDS = 4
res = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
alpha = 1,
type.measure = "auc",
nfolds = NFOLDS,
thresh = 1e-3,
maxit = 1e3)
it_test = test$review %>% prep_fun %>% tok_fun %>%
itoken(ids = test$id, progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(res, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
vocab = create_vocabulary(it_train, ngram = c(1, 2))
print(vocab)
vocab = vocab %>% prune_vocabulary(term_count_min = 10,
doc_proportion_max = 0.5)
print(vocab)
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
res = cv.glmnet(x = dtm_train, y = train[['sentiment']],
family = 'binomial',
alpha = 1,
type.measure = "auc",
nfolds = NFOLDS,
thresh = 1e-3,
maxit = 1e3)
plot(res)
print(names(res))
#AUC (area under curve)
print(max(res$cvm))
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
dtm_test_tfidf  = create_dtm(it_test, vectorizer) %>% transform(tfidf)
res = cv.glmnet(x = dtm_train_tfidf, y = train[['sentiment']],
family = 'binomial',
alpha = 1,
type.measure = "auc",
nfolds = NFOLDS,
thresh = 1e-3,
maxit = 1e3)
print(paste("max AUC =", round(max(res$cvm), 4)))
#Test on hold-out sample
preds = predict(res, dtm_test_tfidf, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
library(tm)
library(text2vec)
stopw = stopwords('en')
stopw = c(stopw,"br","t","s","m","ve","2","d","1")
#Make DTM
data("movie_review")
tokens = movie_review$review %>% tolower %>% word_tokenizer()
it = itoken(tokens)
v = create_vocabulary(it, stopwords = stopw) %>%
prune_vocabulary(term_count_min=5)
vectrzr = vocab_vectorizer(v, grow_dtm = TRUE, skip_grams_window = 5)
dtm = create_dtm(it, vectrzr)
print(dim(dtm))
#Do LDA
lda = LatentDirichletAllocation$new(n_topics=5, v)
lda$fit(dtm,n_iter = 25)
doc_topics = lda$fit_transform(dtm,n_iter = 25)
print(dim(doc_topics))
#Get word vectors by topic
topic_wv = lda$get_word_vectors()
print(dim(topic_wv))
#Plot LDA
library(LDAvis)
lda$plot()
library(igraph)
g = erdos.renyi.game(20,1/10)
g
plot.igraph(g)
print(clusters(g))
clusters(g)$csize %>% mean()
g = erdos.renyi.game(30,0.2)
plot(g)
dd = degree.distribution(g)
dd = as.matrix(dd)
dd
d = as.matrix(seq(0,max(degree(g))))
plot(d,dd,type="l")
plot.igraph(g)
shortest.paths(g,1)
el <- matrix(nc=3, byrow=TRUE,c(0,1,0, 0,2,2, 0,3,1, 1,2,0, 1,4,5, 1,5,2, 2,1,1, 2,3,1, 2,6,1, 3,2,0,
3,6,2, 4,5,2, 4,7,8, 5,2,2, 5,6,1, 5,8,1, 5,9,3, 7,5,1, 7,8,1, 8,9,4) )
el
g = add.edges(graph.empty(10), t(el[,1:2]), weight=el[,3])
plot(g)
g = simplify(g)
V(g)$name = seq(vcount(g))
l = layout.fruchterman.reingold(g)
l = layout.norm(l, -1,1,-1,1)
plot(g, layout=l, vertex.size=10, vertex.label=seq(1,10), vertex.color="#ff000033",
edge.color="grey", edge.arrow.size=0.075, rescale=FALSE,
xlim=range(l[,1]), ylim=range(l[,2]))
l = layout.circle(g)
plot(g, layout=l, vertex.size=10, vertex.label=seq(1,10), vertex.color="#ff000033",
edge.color="grey", edge.arrow.size=0.075, rescale=FALSE,
xlim=range(l[,1]), ylim=range(l[,2]))
